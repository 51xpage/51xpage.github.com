---
layout: post
title: "浅滩拾遗 - python抓网页入库"
description: "通过python抓取数据，并把结构化以后的内容存入mysql"
category: "Python"
modified: 2015-10-01 14:02
tags: "mysql load file python bs4"
---
{% include JB/setup %}
# 1、问题提出
希望抓取某个网站的数据，并最终录入到mysql中去。最顺手的就是Delphi了，想想还是尝试一下新东西，想到用python。比较出名的爬虫工具是scrapy，可惜是python 2的，想想既然学了3，还是用3吧。目标是

* 动态抓取数据
* 解析并存入数据库

# 2、尝试方法

### 2.1 直连数据库
因为无法用scrapy，另外对python爬虫也不熟悉，就想到暴力解析，直接http请求，然后解析返回的html，插入数据库，然后再请求再解析。
这里的核心问题是3个：

1. 如何快速的请求，并拿到结果
2. 如何解析html
3. 如何存入mysql

有几个问题：

1. 需要考虑多线程。
2. 如果自己字符串解析倒也可以，但是会花很多时间在适配上。
3. 数据库一直这么连着也不是个事
4. 如果中途断了，又回头去比对各种数据也比较麻烦
5. 实践的时候，发现连数据比较麻烦，经常断，因为网络问题

另外因为数据量不算太小，一共大概5类数据，最大的16W条，最小的也要2W多条。都是表格形式。所以想了2个办法应对

###2.2 本地文本文件
* 具体页面信息是慢慢爬出来的，通过网页中的列表跳转过去，所以先在本地做了一个索引文件，也就是id文件，最后通过它来拼URL

* 生成的数据先不放数据库，生成一个本地文件，每条记录按行，每个字段用特殊字符分隔

* 所以在设计表结构的时候，增加了2个字段页码和数据id号，为将来更新预留


# 3、最终方案
最终方案是基于上面的方法2做的。最关键是找到了python的解析神器，非常非常帅的神器**BS4**  。整体功能上有如下内容：

* 遍历所有列表。生成本地文件，格式为

	页码：数据id
* 遍历前面生成的id文件，解析数据  
	通过Beautiful Soup 4解析表格  
	解析成功以后把索引文件的中的这部分内容删除

* 插入mysql利用load file命令

{% highlight sql %} 

LOAD DATA LOCAL INFILE '文件名' into table 表名; 
	
{% endhighlight %} 

# 4、收获
* 利用mysql的文件录入命令，需要把字段对齐。不算费劲。  
   即把字段在表中的相对位置弄成和文件一致
   感觉比用Navicat方便，毕竟数据量毕竟大
   用Navicat还需要一个个字段去对齐
   
* 通过命令导入的时候，如果想要导入NULL。可以把值改成**\N**。   
	我是直接用空格代替了……
   
* 如果用Navicat去导入数据，如果第一行（可能不对，反正它会选定一行）。某些字段是没有值的，那在对字段的时候就毕竟麻烦。我的做法是把这些值补上，写个字段名也好。大不了导入进去再改

* 用bs去处理数据的时候，如果一个条件不够，我是另外换个条件遍历了一次，可能有别的好的办法吧。

